{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Kolada Data\n",
    "\n",
    "This notebook fetches actual data values from the Kolada API for specified KPIs, municipalities, and years.\n",
    "\n",
    "**API Endpoints:**\n",
    "- `http://api.kolada.se/v2/data/kpi/{kpi}/municipality/{municipality}/year/{year}`\n",
    "- `http://api.kolada.se/v2/oudata/kpi/{kpi}/ou/{ou}/year/{year}`\n",
    "\n",
    "**Parameters to customize:**\n",
    "- KPI IDs (comma-separated)\n",
    "- Municipality IDs (comma-separated)\n",
    "- Years (comma-separated)\n",
    "\n",
    "**Output:** Kolada data tables in Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "API_BASE_URL = \"http://api.kolada.se/v2\"\n",
    "PER_PAGE = 5000\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "# Default years\n",
    "YEARS = [str(y) for y in range(2010, 2026)]\n",
    "OU_YEARS = [str(y) for y in range(2010, 2026)]\n",
    "\n",
    "# Read all KPI IDs from Lakehouse\n",
    "df_kpi_dim = spark.table(\"dKpi\").toPandas()\n",
    "KPI_IDS = df_kpi_dim[\"id\"].tolist()\n",
    "print(f\"Loaded {len(KPI_IDS)} KPIs from dKpi\")\n",
    "\n",
    "# Filter OU KPIs (has_ou_data == True or 1)\n",
    "OU_KPI_IDS = df_kpi_dim[df_kpi_dim[\"has_ou_data\"] == 1][\"id\"].tolist()\n",
    "print(f\"Loaded {len(OU_KPI_IDS)} OU KPIs from dKpi\")\n",
    "\n",
    "# Leave empty for all municipalities/OUs\n",
    "MUNICIPALITY_IDS = []\n",
    "OU_IDS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data_paginated(url: str) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Fetch data from Kolada API with pagination support.\n",
    "    \n",
    "    Args:\n",
    "        url: Initial API URL\n",
    "        \n",
    "    Returns:\n",
    "        List of data objects\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    page_count = 0\n",
    "    \n",
    "    while url:\n",
    "        try:\n",
    "            response = requests.get(url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            data = response.json()\n",
    "            \n",
    "            if 'values' in data:\n",
    "                all_data.extend(data['values'])\n",
    "                page_count += 1\n",
    "                print(f\"  Page {page_count}: Retrieved {len(data['values'])} items (Total: {len(all_data)})\")\n",
    "            \n",
    "            # Check for next page\n",
    "            url = data.get('next_page', None)\n",
    "            \n",
    "            # Be nice to the API\n",
    "            if url:\n",
    "                time.sleep(0.5)\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"  Error fetching data: {e}\")\n",
    "            break\n",
    "    \n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_kolada_data(kpi_ids: List[str], municipality_ids: Optional[List[str]], years: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch Kolada data for specified parameters.\n",
    "    \n",
    "    Args:\n",
    "        kpi_ids: List of KPI IDs\n",
    "        municipality_ids: List of municipality IDs (None for all)\n",
    "        years: List of years\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with all fetched data\n",
    "    \"\"\"\n",
    "    all_records = []\n",
    "    \n",
    "    # Build URL based on parameters\n",
    "    kpi_param = ','.join(kpi_ids)\n",
    "    year_param = ','.join(years)\n",
    "    \n",
    "    if municipality_ids and len(municipality_ids) > 0:\n",
    "        municipality_param = ','.join(municipality_ids)\n",
    "        url = f\"{API_BASE_URL}/data/kpi/{kpi_param}/municipality/{municipality_param}/year/{year_param}?per_page={PER_PAGE}\"\n",
    "    else:\n",
    "        url = f\"{API_BASE_URL}/data/kpi/{kpi_param}/year/{year_param}?per_page={PER_PAGE}\"\n",
    "    \n",
    "    print(f\"Fetching data from: {url}\")\n",
    "    \n",
    "    data = fetch_data_paginated(url)\n",
    "    \n",
    "    # Flatten the data structure\n",
    "    for item in data:\n",
    "        kpi = item.get('kpi')\n",
    "        municipality = item.get('municipality')\n",
    "        period = item.get('period')\n",
    "        \n",
    "        # Each item can have multiple values (by gender)\n",
    "        if 'values' in item:\n",
    "            for value_item in item['values']:\n",
    "                record = {\n",
    "                    'kpi': kpi,\n",
    "                    'municipality': municipality,\n",
    "                    'period': period,\n",
    "                    'gender': value_item.get('gender'),\n",
    "                    'value': value_item.get('value'),\n",
    "                    'count': value_item.get('count'),\n",
    "                    'status': value_item.get('status'),\n",
    "                    'ingestion_timestamp': datetime.now(),\n",
    "                    'source_system': 'Kolada API',\n",
    "                    'ou_id': f\"NO_OU_{municipality}\",  # Placeholder for municipality-level data\n",
    "                }\n",
    "                all_records.append(record)\n",
    "    \n",
    "    if all_records:\n",
    "        return pd.DataFrame(all_records)\n",
    "    else:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_ou_data(kpi_ids: List[str], ou_ids: Optional[List[str]], years: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch Kolada organizational unit data for specified parameters.\n",
    "    \n",
    "    Args:\n",
    "        kpi_ids: List of KPI IDs\n",
    "        ou_ids: List of OU IDs (None for all)\n",
    "        years: List of years\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with all fetched data\n",
    "    \"\"\"\n",
    "    all_records = []\n",
    "    \n",
    "    # Build URL based on parameters\n",
    "    kpi_param = ','.join(kpi_ids)\n",
    "    year_param = ','.join(years)\n",
    "    \n",
    "    if ou_ids and len(ou_ids) > 0:\n",
    "        ou_param = ','.join(ou_ids)\n",
    "        url = f\"{API_BASE_URL}/oudata/kpi/{kpi_param}/ou/{ou_param}/year/{year_param}?per_page={PER_PAGE}\"\n",
    "    else:\n",
    "        url = f\"{API_BASE_URL}/oudata/kpi/{kpi_param}/year/{year_param}?per_page={PER_PAGE}\"\n",
    "    \n",
    "    print(f\"Fetching OU data from: {url}\")\n",
    "    \n",
    "    data = fetch_data_paginated(url)\n",
    "    \n",
    "    # Flatten the data structure\n",
    "    for item in data:\n",
    "        kpi = item.get('kpi')\n",
    "        ou = item.get('ou')\n",
    "        period = item.get('period')\n",
    "        \n",
    "        # Each item can have multiple values (by gender)\n",
    "        if 'values' in item:\n",
    "            for value_item in item['values']:\n",
    "                record = {\n",
    "                    'kpi': kpi,\n",
    "                    'ou_id': ou,\n",
    "                    'municipality': None,  # Will be looked up from dOrganizationalUnit\n",
    "                    'period': period,\n",
    "                    'gender': value_item.get('gender'),\n",
    "                    'value': value_item.get('value'),\n",
    "                    'count': value_item.get('count'),\n",
    "                    'status': value_item.get('status'),\n",
    "                    'ingestion_timestamp': datetime.now(),\n",
    "                    'source_system': 'Kolada API'\n",
    "                }\n",
    "                all_records.append(record)\n",
    "    \n",
    "    if all_records:\n",
    "        return pd.DataFrame(all_records)\n",
    "    else:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch municipality data in batches\n",
    "print(\"=\"*60)\n",
    "print(\"Fetching Municipality Data\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total KPIs: {len(KPI_IDS)}\")\n",
    "print(f\"Municipalities: {MUNICIPALITY_IDS if MUNICIPALITY_IDS else 'All'}\")\n",
    "print(f\"Years: {YEARS}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print()\n",
    "\n",
    "all_frames = []\n",
    "for i in range(0, len(KPI_IDS), BATCH_SIZE):\n",
    "    batch = KPI_IDS[i:i+BATCH_SIZE]\n",
    "    print(f\"\\nBatch {i//BATCH_SIZE + 1}: Fetching {len(batch)} KPIs ({batch[:3]}...)\")\n",
    "    df_batch = fetch_kolada_data(batch, MUNICIPALITY_IDS if MUNICIPALITY_IDS else None, YEARS)\n",
    "    if not df_batch.empty:\n",
    "        all_frames.append(df_batch)\n",
    "        print(f\"  -> Fetched {len(df_batch)} data points\")\n",
    "\n",
    "df_data = pd.concat(all_frames, ignore_index=True) if all_frames else pd.DataFrame()\n",
    "\n",
    "if not df_data.empty:\n",
    "    print(f\"\\n\u2713 Total data points fetched: {len(df_data)}\")\n",
    "    print(f\"DataFrame shape: {df_data.shape}\")\n",
    "    print(f\"Columns: {list(df_data.columns)}\")\n",
    "    display(df_data.head(10))\n",
    "else:\n",
    "    print(\"No data retrieved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality summary\n",
    "if not df_data.empty:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DATA SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nTotal data points: {len(df_data)}\")\n",
    "    print(f\"\\nData points by KPI:\")\n",
    "    print(df_data['kpi'].value_counts())\n",
    "    \n",
    "    print(f\"\\nData points by period:\")\n",
    "    print(df_data['period'].value_counts().sort_index())\n",
    "    \n",
    "    print(f\"\\nData points by gender:\")\n",
    "    print(df_data['gender'].value_counts())\n",
    "    \n",
    "    print(f\"\\nNull values:\")\n",
    "    print(df_data['value'].isnull().sum(), \"out of\", len(df_data))\n",
    "    \n",
    "    if MUNICIPALITY_IDS:\n",
    "        print(f\"\\nData points by municipality:\")\n",
    "        print(df_data['municipality'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Organizational Unit Data\n",
    "\n",
    "OU data is now enabled by default and will be merged with municipality data into a unified fact table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch OU data in batches\n",
    "print(\"=\"*60)\n",
    "print(\"Fetching Organizational Unit Data\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total OU KPIs: {len(OU_KPI_IDS)}\")\n",
    "print(f\"OUs: {OU_IDS if OU_IDS else 'All'}\")\n",
    "print(f\"Years: {OU_YEARS}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print()\n",
    "\n",
    "all_ou_frames = []\n",
    "for i in range(0, len(OU_KPI_IDS), BATCH_SIZE):\n",
    "    batch = OU_KPI_IDS[i:i+BATCH_SIZE]\n",
    "    print(f\"\\nBatch {i//BATCH_SIZE + 1}: Fetching {len(batch)} OU KPIs ({batch[:3]}...)\")\n",
    "    df_batch = fetch_ou_data(batch, OU_IDS if OU_IDS else None, OU_YEARS)\n",
    "    if not df_batch.empty:\n",
    "        all_ou_frames.append(df_batch)\n",
    "        print(f\"  -> Fetched {len(df_batch)} OU data points\")\n",
    "\n",
    "df_ou_data = pd.concat(all_ou_frames, ignore_index=True) if all_ou_frames else pd.DataFrame()\n",
    "\n",
    "if not df_ou_data.empty:\n",
    "    print(f\"\\n\u2713 Total OU data points fetched: {len(df_ou_data)}\")\n",
    "    print(f\"DataFrame shape: {df_ou_data.shape}\")\n",
    "    print(f\"Columns: {list(df_ou_data.columns)}\")\n",
    "    display(df_ou_data.head(10))\n",
    "else:\n",
    "    print(\"No OU data retrieved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look up municipality for OU data from dOrganizationalUnit table\n",
    "if not df_ou_data.empty:\n",
    "    print(\"\\nLooking up municipality for OU data...\")\n",
    "    try:\n",
    "        # Load OU dimension table\n",
    "        df_ou_dim = spark.table(\"dOrganizationalUnit\").toPandas()\n",
    "        ou_to_muni = df_ou_dim.set_index('id')['municipality'].to_dict()\n",
    "        \n",
    "        # Look up municipality for each OU\n",
    "        df_ou_data['municipality'] = df_ou_data['ou_id'].map(ou_to_muni)\n",
    "        print(f\"  \u2713 Looked up municipality for {len(df_ou_data)} OU records\")\n",
    "    except Exception as e:\n",
    "        print(f\"  \u26a0 Could not look up municipality: {e}\")\n",
    "        print(\"  Make sure notebook 02 has been run to create dOrganizationalUnit table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine municipality and OU data into unified fact table\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Creating Unified Fact Table\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "frames_to_combine = []\n",
    "if not df_data.empty:\n",
    "    print(f\"Municipality data: {len(df_data)} rows\")\n",
    "    frames_to_combine.append(df_data)\n",
    "    \n",
    "if not df_ou_data.empty:\n",
    "    print(f\"OU data: {len(df_ou_data)} rows\")\n",
    "    frames_to_combine.append(df_ou_data)\n",
    "\n",
    "if frames_to_combine:\n",
    "    df_combined = pd.concat(frames_to_combine, ignore_index=True)\n",
    "    print(f\"\\nCombined data: {len(df_combined)} rows\")\n",
    "    print(f\"Columns: {list(df_combined.columns)}\")\n",
    "else:\n",
    "    print(\"No data to combine\")\n",
    "    df_combined = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add surrogate keys from dimension tables\n",
    "if not df_combined.empty:\n",
    "    print(\"\\nAdding surrogate keys...\")\n",
    "    \n",
    "    try:\n",
    "        # Load dimension tables\n",
    "        df_kpi_dim = spark.table(\"dKpi\").toPandas()\n",
    "        df_muni_dim = spark.table(\"dMunicipality\").toPandas()\n",
    "        df_ou_dim = spark.table(\"dOrganizationalUnit\").toPandas()\n",
    "        \n",
    "        # Create lookup dictionaries\n",
    "        kpi_lookup = df_kpi_dim.set_index('id')['kpi_key'].to_dict()\n",
    "        muni_lookup = df_muni_dim.set_index('id')['municipality_key'].to_dict()\n",
    "        ou_lookup = df_ou_dim.set_index('id')['ou_key'].to_dict()\n",
    "        \n",
    "        # Add surrogate keys\n",
    "        df_combined['kpi_key'] = df_combined['kpi'].map(kpi_lookup)\n",
    "        df_combined['municipality_key'] = df_combined['municipality'].map(muni_lookup)\n",
    "        df_combined['ou_key'] = df_combined['ou_id'].map(ou_lookup)\n",
    "        \n",
    "        print(f\"  \u2713 Added kpi_key (matched {df_combined['kpi_key'].notna().sum()}/{len(df_combined)} rows)\")\n",
    "        print(f\"  \u2713 Added municipality_key (matched {df_combined['municipality_key'].notna().sum()}/{len(df_combined)} rows)\")\n",
    "        print(f\"  \u2713 Added ou_key (matched {df_combined['ou_key'].notna().sum()}/{len(df_combined)} rows)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  \u26a0 Could not add surrogate keys: {e}\")\n",
    "        print(\"  Make sure notebooks 01 and 02 have been run to create dimension tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save unified fact table to Lakehouse\n",
    "if not df_combined.empty:\n",
    "    spark_df = spark.createDataFrame(df_combined)\n",
    "    spark_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"fKoladaData\")\n",
    "    \n",
    "    print(f\"\\n\u2713 Successfully wrote {len(df_combined)} rows to fKoladaData\")\n",
    "    print(\"\\nSample of unified fact table:\")\n",
    "    display(df_combined.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality summary for unified fact table\n",
    "if not df_combined.empty:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"UNIFIED FACT TABLE SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nTotal data points: {len(df_combined)}\")\n",
    "    \n",
    "    # Count by data source (based on ou_id pattern)\n",
    "    municipality_count = df_combined['ou_id'].str.startswith('NO_OU_').sum()\n",
    "    ou_count = len(df_combined) - municipality_count\n",
    "    print(f\"\\nData points by source:\")\n",
    "    print(f\"  Municipality-level: {municipality_count}\")\n",
    "    print(f\"  OU-level: {ou_count}\")\n",
    "    \n",
    "    print(f\"\\nData points by KPI:\")\n",
    "    print(df_combined['kpi'].value_counts())\n",
    "    \n",
    "    print(f\"\\nData points by period:\")\n",
    "    print(df_combined['period'].value_counts().sort_index())\n",
    "    \n",
    "    print(f\"\\nData points by gender:\")\n",
    "    print(df_combined['gender'].value_counts())\n",
    "    \n",
    "    print(f\"\\nNull values in value column:\")\n",
    "    print(df_combined['value'].isnull().sum(), \"out of\", len(df_combined))\n",
    "    \n",
    "    print(f\"\\nSurrogate key coverage:\")\n",
    "    print(f\"  kpi_key: {df_combined['kpi_key'].notna().sum()}/{len(df_combined)}\")\n",
    "    print(f\"  municipality_key: {df_combined['municipality_key'].notna().sum()}/{len(df_combined)}\")\n",
    "    print(f\"  ou_key: {df_combined['ou_key'].notna().sum()}/{len(df_combined)}\")"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}