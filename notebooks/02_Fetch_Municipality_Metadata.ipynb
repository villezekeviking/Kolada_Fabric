{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fetch Municipality and Groups Metadata from Kolada API\n",
        "\n",
        "This notebook fetches municipality, municipality groups, and organizational units metadata from the Kolada API and stores them in the Lakehouse.\n",
        "\n",
        "**API Endpoints:**\n",
        "- `http://api.kolada.se/v2/municipality`\n",
        "- `http://api.kolada.se/v2/municipality_groups`\n",
        "- `http://api.kolada.se/v2/ou`\n",
        "- `http://api.kolada.se/v2/kpi_groups`\n",
        "\n",
        "**Output:** Multiple metadata tables in Lakehouse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "API_BASE_URL = \"http://api.kolada.se/v2\"\n",
        "PER_PAGE = 5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fetch_metadata(endpoint, table_name):\n",
        "    \"\"\"\n",
        "    Generic function to fetch metadata from Kolada API with pagination.\n",
        "    \n",
        "    Args:\n",
        "        endpoint: API endpoint (e.g., 'municipality', 'kpi_groups')\n",
        "        table_name: Name for the output table\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame: Pandas DataFrame with the fetched data\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Fetching {endpoint} metadata\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    all_data = []\n",
        "    url = f\"{API_BASE_URL}/{endpoint}?per_page={PER_PAGE}\"\n",
        "    \n",
        "    page_count = 0\n",
        "    \n",
        "    while url:\n",
        "        try:\n",
        "            print(f\"Fetching page {page_count + 1}...\")\n",
        "            response = requests.get(url, timeout=30)\n",
        "            response.raise_for_status()\n",
        "            \n",
        "            data = response.json()\n",
        "            \n",
        "            if 'values' in data:\n",
        "                all_data.extend(data['values'])\n",
        "                print(f\"  Retrieved {len(data['values'])} items (Total: {len(all_data)})\")\n",
        "            \n",
        "            # Check for next page\n",
        "            url = data.get('next_page', None)\n",
        "            page_count += 1\n",
        "            \n",
        "            # Be nice to the API\n",
        "            if url:\n",
        "                time.sleep(0.5)\n",
        "                \n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching data: {e}\")\n",
        "            break\n",
        "    \n",
        "    print(f\"Total items fetched: {len(all_data)}\")\n",
        "    \n",
        "    if all_data:\n",
        "        df = pd.DataFrame(all_data)\n",
        "        df['ingestion_timestamp'] = datetime.now()\n",
        "        df['source_system'] = 'Kolada API'\n",
        "        return df\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_to_lakehouse(df, table_name):\n",
        "    \"\"\"\n",
        "    Save DataFrame to Lakehouse as Delta table.\n",
        "    \n",
        "    Args:\n",
        "        df: Pandas DataFrame to save\n",
        "        table_name: Name of the table\n",
        "    \"\"\"\n",
        "    if df is None or df.empty:\n",
        "        print(f\"  \u2717 No data to save for {table_name}\")\n",
        "        return\n",
        "    \n",
        "    try:\n",
        "        # Write to Delta table in Lakehouse\n",
        "        table_path = f\"Tables/{table_name}\"\n",
        "        \n",
        "        # Use overwrite mode for full refresh\n",
        "        spark.createDataFrame(df).write.format(\"delta\").mode(\"overwrite\").save(table_path)\n",
        "        \n",
        "        print(f\"  \u2713 Successfully wrote {len(df)} rows to {table_name}\")\n",
        "        print(f\"    Table path: {table_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  \u2717 Error writing to Lakehouse: {e}\")\n",
        "        print(f\"    Attempting to save as Parquet file instead...\")\n",
        "        \n",
        "        try:\n",
        "            # Fallback to Files section\n",
        "            file_path = f\"Files/{table_name}.parquet\"\n",
        "            df.to_parquet(file_path, index=False)\n",
        "            print(f\"    \u2713 Saved to {file_path}\")\n",
        "        except Exception as e2:\n",
        "            print(f\"    \u2717 Error saving Parquet: {e2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fetch Municipality metadata\n",
        "df_municipality = fetch_metadata('municipality', 'municipality_metadata')\n",
        "if df_municipality is not None:\n",
        "    print(f\"\\nDataFrame shape: {df_municipality.shape}\")\n",
        "    print(f\"Columns: {list(df_municipality.columns)}\")\n",
        "    display(df_municipality.head())\n",
        "    save_to_lakehouse(df_municipality, 'municipality_metadata')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fetch Municipality Groups metadata\n",
        "df_municipality_groups = fetch_metadata('municipality_groups', 'municipality_groups_metadata')\n",
        "if df_municipality_groups is not None:\n",
        "    print(f\"\\nDataFrame shape: {df_municipality_groups.shape}\")\n",
        "    print(f\"Columns: {list(df_municipality_groups.columns)}\")\n",
        "    \n",
        "    # Flatten the members array if it exists\n",
        "    if 'members' in df_municipality_groups.columns:\n",
        "        # Store the main groups table\n",
        "        df_groups_main = df_municipality_groups[['id', 'title', 'ingestion_timestamp', 'source_system']].copy()\n",
        "        save_to_lakehouse(df_groups_main, 'municipality_groups_metadata')\n",
        "        \n",
        "        # Create a separate table for group members\n",
        "        members_list = []\n",
        "        for idx, row in df_municipality_groups.iterrows():\n",
        "            if row['members'] and isinstance(row['members'], list):\n",
        "                for member in row['members']:\n",
        "                    members_list.append({\n",
        "                        'group_id': row['id'],\n",
        "                        'group_title': row['title'],\n",
        "                        'member_id': member.get('id'),\n",
        "                        'member_title': member.get('title'),\n",
        "                        'ingestion_timestamp': row['ingestion_timestamp'],\n",
        "                        'source_system': row['source_system']\n",
        "                    })\n",
        "        \n",
        "        if members_list:\n",
        "            df_members = pd.DataFrame(members_list)\n",
        "            save_to_lakehouse(df_members, 'municipality_group_members')\n",
        "    else:\n",
        "        save_to_lakehouse(df_municipality_groups, 'municipality_groups_metadata')\n",
        "    \n",
        "    display(df_municipality_groups.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fetch KPI Groups metadata\n",
        "df_kpi_groups = fetch_metadata('kpi_groups', 'kpi_groups_metadata')\n",
        "if df_kpi_groups is not None:\n",
        "    print(f\"\\nDataFrame shape: {df_kpi_groups.shape}\")\n",
        "    print(f\"Columns: {list(df_kpi_groups.columns)}\")\n",
        "    \n",
        "    # Flatten the members array if it exists\n",
        "    if 'members' in df_kpi_groups.columns:\n",
        "        # Store the main groups table\n",
        "        df_kpi_groups_main = df_kpi_groups[['id', 'title', 'ingestion_timestamp', 'source_system']].copy()\n",
        "        save_to_lakehouse(df_kpi_groups_main, 'kpi_groups_metadata')\n",
        "        \n",
        "        # Create a separate table for group members\n",
        "        kpi_members_list = []\n",
        "        for idx, row in df_kpi_groups.iterrows():\n",
        "            if row['members'] and isinstance(row['members'], list):\n",
        "                for member in row['members']:\n",
        "                    kpi_members_list.append({\n",
        "                        'group_id': row['id'],\n",
        "                        'group_title': row['title'],\n",
        "                        'kpi_id': member.get('id'),\n",
        "                        'kpi_title': member.get('title'),\n",
        "                        'ingestion_timestamp': row['ingestion_timestamp'],\n",
        "                        'source_system': row['source_system']\n",
        "                    })\n",
        "        \n",
        "        if kpi_members_list:\n",
        "            df_kpi_members = pd.DataFrame(kpi_members_list)\n",
        "            save_to_lakehouse(df_kpi_members, 'kpi_group_members')\n",
        "    else:\n",
        "        save_to_lakehouse(df_kpi_groups, 'kpi_groups_metadata')\n",
        "    \n",
        "    display(df_kpi_groups.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fetch Organizational Units metadata\n",
        "df_ou = fetch_metadata('ou', 'organizational_units_metadata')\n",
        "if df_ou is not None:\n",
        "    print(f\"\\nDataFrame shape: {df_ou.shape}\")\n",
        "    print(f\"Columns: {list(df_ou.columns)}\")\n",
        "    display(df_ou.head())\n",
        "    save_to_lakehouse(df_ou, 'organizational_units_metadata')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if df_municipality is not None:\n",
        "    print(f\"\\nMunicipalities: {len(df_municipality)} total\")\n",
        "    print(f\"  By type: {df_municipality['type'].value_counts().to_dict()}\")\n",
        "\n",
        "if df_municipality_groups is not None:\n",
        "    print(f\"\\nMunicipality Groups: {len(df_municipality_groups)} total\")\n",
        "\n",
        "if df_kpi_groups is not None:\n",
        "    print(f\"\\nKPI Groups: {len(df_kpi_groups)} total\")\n",
        "\n",
        "if df_ou is not None:\n",
        "    print(f\"\\nOrganizational Units: {len(df_ou)} total\")"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}